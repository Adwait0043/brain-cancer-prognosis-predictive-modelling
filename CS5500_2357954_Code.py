# -*- coding: utf-8 -*-
"""CS5500_2357954_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_zMxF9IC0JJhYtw0L0CAJsu-6_L0_YQ
"""

import zipfile
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.image as mpimg
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Defining the path
DataPath = '/content/drive/MyDrive/archive'
DataDirectory = '/content/drive'

# Mounting the data
from google.colab import drive
drive.mount('/content/drive')

# Extracting the ZIP file
with zipfile.ZipFile('/content/drive/MyDrive/archive.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/drive/MyDrive')

## Getting the list of all files in 'DataDirectory' and printing it.
DataFiles = os.listdir(DataDirectory)
print(DataFiles)

#Creating the path to 'ImageDirectory' and listing its contents.

ImageDirectory = os.path.join('/content/drive/MyDrive/archive/Brain Tumor')
ImgDirectoryContents = os.listdir(ImageDirectory)

# Printing the first few data
ImgDirectoryContents[:10]

# Loading the CSV file
BrainTumor = pd.read_csv('/content/drive/MyDrive/archive/Brain Tumor.csv')
BTDataset3 = pd.read_csv('/content/drive/MyDrive/archive/bt_dataset_t3.csv')

# Information about the BTDataset3
BTDataset3.info()

# First few rows of the BTDataset3
print(BTDataset3.head())

# Last few rows of the BTDataset3
print(BTDataset3.tail())

# Descriptive statistics of the BTDataset3
print(BTDataset3.describe())

# Checking for null values
print(BTDataset3.isnull().sum())

# Removing null values from the BTDataset3
BTDataset3 = BTDataset3.dropna()

# Listing the columns of the BTDataset3
print(BTDataset3.columns)

# Getting information about the BrainTumor
print(BrainTumor.info())

# Printing the first few data
BrainTumor.head()

# Displaying the last few rows
print(BrainTumor.tail())

# Describing the numerical columns
print(BrainTumor.describe())

# Checking for null values
print(BrainTumor.isnull().sum())

# Listing the columns
print(BrainTumor.columns)

# Plotting Variance vs Standard Deviation
BrainTumor.plot(kind='scatter', x='Variance', y='Standard Deviation', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

# Plotting Mean vs Variance
BrainTumor.plot(kind='scatter', x='Mean', y='Variance', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

# Plotting Standard Deviation vs Entropy
BrainTumor.plot(kind='scatter', x='Standard Deviation', y='Entropy', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

# Plotting Standard Deviation
BrainTumor['Standard Deviation'].plot(kind='hist', bins=20, title='Standard Deviation')
plt.gca().spines[['top', 'right',]].set_visible(False)

# Visualize distributions
sns.histplot(BrainTumor['Mean'], kde=True)
plt.title('Distribution of Mean')
plt.show()

# Plotting Class Distribution
sns.countplot(x='Class', data=BrainTumor)
plt.title('Class Distribution')
plt.show()

# Displaying the first few images
plt.figure(figsize=(10, 10))
for i in range(9):
  files = os.listdir(os.path.join(ImageDirectory, ImgDirectoryContents[0]))
  file = files[i]
  img_path = os.path.join(ImageDirectory, ImgDirectoryContents[0], file)
  img = mpimg.imread(img_path)
  plt.subplot(3, 3, i + 1)
  plt.imshow(img)
  plt.axis('off')
plt.show()

# Defining the target and features
X = BrainTumor.drop(['Image', 'Class'], axis=1)
y = BrainTumor['Class']

# Encoding the target variable
le = LabelEncoder()
yEncoded = le.fit_transform(y)

# Splitting the data
X_train, X_test, y_train, y_test = train_test_split(X, yEncoded, test_size=0.2, random_state=42)

# Normalizing the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Training a Random Forest model
RandomModel = RandomForestClassifier(n_estimators=100, random_state=42)
RandomModel.fit(X_train, y_train)

# Predictions
y_pred = RandomModel.predict(X_test)

# Printing Accuracy of the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f} %")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Building a simple neural network
model = Sequential([
    Dense(128, input_dim=X_train.shape[1], activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compiling the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Training the model
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))



# Evaluating the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy of the Model: {test_acc*100:.2f} %")

# Evaluation of model on testing data
y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)

# Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plot training history
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.show()

# Preparing Image Data
image_size = (128, 128)
batch_size = 32

# Initialising training and validation data generator
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)
train_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/archive/Brain Tumor',
    target_size=image_size,
    batch_size=batch_size,
    class_mode='binary',
    subset='training'
)

validation_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/archive/Brain Tumor',
    target_size=image_size,
    batch_size=batch_size,
    class_mode='binary',
    subset='validation'
)

# Building a CNN model
cnn_model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Compiling the CNN model
cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Training the CNN model
cnn_history = cnn_model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=10,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // batch_size
)

# Evaluating the CNN model
test_loss, test_acc = cnn_model.evaluate(validation_generator)
print(f"Test Accuracy of the CNN Model: {test_acc*100:.2f} %")

# Getting some images and labels
images, labels = next(validation_generator)

# Making predictions on testing images
predictions = cnn_model.predict(images)

# Threshold predictions to get class labels
predicted_labels = np.ones_like(predictions, dtype=int)

# Plotting images with actual and predicted labels
plt.figure(figsize=(15, 15))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(images[i])
    actual_label = "Tumor" if labels[i] == 1 else "No Tumor"
    predicted_label = "Tumor" if predictions[i] > 0.5 else "No Tumor"
    plt.title(f"Actual: {actual_label}\nPredicted: {predicted_label}")
    plt.axis('off')
plt.show()